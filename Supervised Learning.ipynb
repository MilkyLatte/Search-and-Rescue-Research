{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMaps():\n",
    "    with open('maps1.json', 'r') as f:\n",
    "        a = json.load(f)\n",
    "    return copy.deepcopy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 0, 0, 2, 4, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "maps = loadMaps()\n",
    "\n",
    "print(maps[\"maps\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2b8a7e9828>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(maps[\"maps\"][0]).reshape(5,5), cmap='gist_gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(flattened):\n",
    "    for i in range(len(flattened)):\n",
    "        minimum = min(flattened)\n",
    "        maximum = max(flattened)\n",
    "        flattened[i] = (flattened[i]-minimum)/(maximum - minimum)\n",
    "        \n",
    "    return flattened\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.75, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "normalized = []\n",
    "for i in maps[\"maps\"]:\n",
    "    normalized.append(normalize(i))\n",
    "print(normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newMaps = {\n",
    "#     \"maps\":[],\n",
    "#     \"moves\":[]\n",
    "# }\n",
    "\n",
    "# newMaps[\"maps\"] = normalized\n",
    "# newMaps[\"moves\"] = maps[\"moves\"]\n",
    "\n",
    "# def writeMaps(maps):\n",
    "#     with open('normalizedMaps.json', 'w') as f:\n",
    "#         a = json.dump(maps, f, separators=(',', ': '), indent=4)\n",
    "# writeMaps(newMaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milkylatte/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2b8a818be0>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import cv2\n",
    "npMaps = np.array(maps[\"maps\"])\n",
    "scaler = MinMaxScaler()\n",
    "hey = scaler.fit_transform(npMaps[0].reshape(5,5))\n",
    "\n",
    "plt.imshow(np.array(normalized[0]).reshape(5,5),cmap='gist_gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milkylatte/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "npSolution = np.array(maps[\"moves\"]).reshape(len(maps[\"moves\"]), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded = onehot_encoder.fit_transform(npSolution)\n",
    "\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(normalized), encoded, test_size=0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2dc48e4a58>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[3].reshape(5,5), cmap=\"gist_gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(tf.float32, shape=[None, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = tf.Variable(tf.zeros([400, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = tf.Variable(tf.zeros([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = tf.placeholder(tf.float32, [None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for step in range(10000):\n",
    "#         i = np.random.randint(0, len(X_train)-100)\n",
    "#         sess.run(train, feed_dict={x: X_train[i:i+100], y_true: y_train[i:i+100]})\n",
    "        \n",
    "#     matches = tf.equal(tf.argmax(y, 1), tf.argmax(y_true, 1))\n",
    "#     acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "#     print(sess.run(acc, feed_dict={x:X_test, y_true: y_test}))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    # x --> [batch, H, W, Channels]\n",
    "    # W --> [filter H, filter W, Channels IN, Channels OUT]\n",
    "    \n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2by2(x):\n",
    "    # x --> [batch, h, w, c]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,5,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x_image, shape=[5,5,1,25])\n",
    "#convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2 = convolutional_layer(convo_1, shape=[5,5,25,10])\n",
    "# convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2_flat = tf.reshape(convo_2,[-1,5*5*10])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for step in range(10000):\n",
    "#         i = np.random.randint(0, len(X_train)-100)\n",
    "#         sess.run(train, feed_dict={x: X_train[i:i+100], y_true: y_train[i:i+100]})\n",
    "        \n",
    "#     matches = tf.equal(tf.argmax(y, 1), tf.argmax(y_true, 1))\n",
    "#     acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "#     print(sess.run(acc, feed_dict={x:X_test, y_true: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON STEP: 0\n",
      "ACCURACY: \n",
      "0.24616858\n",
      "ON STEP: 100\n",
      "ACCURACY: \n",
      "0.25\n",
      "ON STEP: 200\n",
      "ACCURACY: \n",
      "0.25191572\n",
      "ON STEP: 300\n",
      "ACCURACY: \n",
      "0.25095785\n",
      "ON STEP: 400\n",
      "ACCURACY: \n",
      "0.26819924\n",
      "ON STEP: 500\n",
      "ACCURACY: \n",
      "0.2710728\n",
      "ON STEP: 600\n",
      "ACCURACY: \n",
      "0.24233717\n",
      "ON STEP: 700\n",
      "ACCURACY: \n",
      "0.2413793\n",
      "ON STEP: 800\n",
      "ACCURACY: \n",
      "0.25191572\n",
      "ON STEP: 900\n",
      "ACCURACY: \n",
      "0.24521072\n",
      "ON STEP: 1000\n",
      "ACCURACY: \n",
      "0.25766283\n",
      "ON STEP: 1100\n",
      "ACCURACY: \n",
      "0.25478926\n",
      "ON STEP: 1200\n",
      "ACCURACY: \n",
      "0.25574714\n",
      "ON STEP: 1300\n",
      "ACCURACY: \n",
      "0.25862068\n",
      "ON STEP: 1400\n",
      "ACCURACY: \n",
      "0.2605364\n",
      "ON STEP: 1500\n",
      "ACCURACY: \n",
      "0.27969348\n",
      "ON STEP: 1600\n",
      "ACCURACY: \n",
      "0.27394637\n",
      "ON STEP: 1700\n",
      "ACCURACY: \n",
      "0.27203065\n",
      "ON STEP: 1800\n",
      "ACCURACY: \n",
      "0.27873564\n",
      "ON STEP: 1900\n",
      "ACCURACY: \n",
      "0.28256705\n",
      "ON STEP: 2000\n",
      "ACCURACY: \n",
      "0.27873564\n",
      "ON STEP: 2100\n",
      "ACCURACY: \n",
      "0.28065133\n",
      "ON STEP: 2200\n",
      "ACCURACY: \n",
      "0.28448275\n",
      "ON STEP: 2300\n",
      "ACCURACY: \n",
      "0.28735632\n",
      "ON STEP: 2400\n",
      "ACCURACY: \n",
      "0.29118773\n",
      "ON STEP: 2500\n",
      "ACCURACY: \n",
      "0.27490422\n",
      "ON STEP: 2600\n",
      "ACCURACY: \n",
      "0.28448275\n",
      "ON STEP: 2700\n",
      "ACCURACY: \n",
      "0.28544062\n",
      "ON STEP: 2800\n",
      "ACCURACY: \n",
      "0.2902299\n",
      "ON STEP: 2900\n",
      "ACCURACY: \n",
      "0.28065133\n",
      "ON STEP: 3000\n",
      "ACCURACY: \n",
      "0.2768199\n",
      "ON STEP: 3100\n",
      "ACCURACY: \n",
      "0.29980844\n",
      "ON STEP: 3200\n",
      "ACCURACY: \n",
      "0.29214558\n",
      "ON STEP: 3300\n",
      "ACCURACY: \n",
      "0.2768199\n",
      "ON STEP: 3400\n",
      "ACCURACY: \n",
      "0.29214558\n",
      "ON STEP: 3500\n",
      "ACCURACY: \n",
      "0.29310346\n",
      "ON STEP: 3600\n",
      "ACCURACY: \n",
      "0.28927204\n",
      "ON STEP: 3700\n",
      "ACCURACY: \n",
      "0.2816092\n",
      "ON STEP: 3800\n",
      "ACCURACY: \n",
      "0.28735632\n",
      "ON STEP: 3900\n",
      "ACCURACY: \n",
      "0.29501915\n",
      "ON STEP: 4000\n",
      "ACCURACY: \n",
      "0.29789272\n",
      "ON STEP: 4100\n",
      "ACCURACY: \n",
      "0.30268198\n",
      "ON STEP: 4200\n",
      "ACCURACY: \n",
      "0.29597703\n",
      "ON STEP: 4300\n",
      "ACCURACY: \n",
      "0.30363986\n",
      "ON STEP: 4400\n",
      "ACCURACY: \n",
      "0.29501915\n",
      "ON STEP: 4500\n",
      "ACCURACY: \n",
      "0.30268198\n",
      "ON STEP: 4600\n",
      "ACCURACY: \n",
      "0.31226054\n",
      "ON STEP: 4700\n",
      "ACCURACY: \n",
      "0.3170498\n",
      "ON STEP: 4800\n",
      "ACCURACY: \n",
      "0.30172414\n",
      "ON STEP: 4900\n",
      "ACCURACY: \n",
      "0.28927204\n",
      "ON STEP: 5000\n",
      "ACCURACY: \n",
      "0.3065134\n",
      "ON STEP: 5100\n",
      "ACCURACY: \n",
      "0.3113027\n",
      "ON STEP: 5200\n",
      "ACCURACY: \n",
      "0.30363986\n",
      "ON STEP: 5300\n",
      "ACCURACY: \n",
      "0.31226054\n",
      "ON STEP: 5400\n",
      "ACCURACY: \n",
      "0.32662836\n",
      "ON STEP: 5500\n",
      "ACCURACY: \n",
      "0.29693487\n",
      "ON STEP: 5600\n",
      "ACCURACY: \n",
      "0.31034482\n",
      "ON STEP: 5700\n",
      "ACCURACY: \n",
      "0.29693487\n",
      "ON STEP: 5800\n",
      "ACCURACY: \n",
      "0.31034482\n",
      "ON STEP: 5900\n",
      "ACCURACY: \n",
      "0.3007663\n",
      "ON STEP: 6000\n",
      "ACCURACY: \n",
      "0.29885057\n",
      "ON STEP: 6100\n",
      "ACCURACY: \n",
      "0.29789272\n",
      "ON STEP: 6200\n",
      "ACCURACY: \n",
      "0.3045977\n",
      "ON STEP: 6300\n",
      "ACCURACY: \n",
      "0.29980844\n",
      "ON STEP: 6400\n",
      "ACCURACY: \n",
      "0.31417623\n",
      "ON STEP: 6500\n",
      "ACCURACY: \n",
      "0.30747128\n",
      "ON STEP: 6600\n",
      "ACCURACY: \n",
      "0.30842912\n",
      "ON STEP: 6700\n",
      "ACCURACY: \n",
      "0.3007663\n",
      "ON STEP: 6800\n",
      "ACCURACY: \n",
      "0.31034482\n",
      "ON STEP: 6900\n",
      "ACCURACY: \n",
      "0.3151341\n",
      "ON STEP: 7000\n",
      "ACCURACY: \n",
      "0.30172414\n",
      "ON STEP: 7100\n",
      "ACCURACY: \n",
      "0.29693487\n",
      "ON STEP: 7200\n",
      "ACCURACY: \n",
      "0.30172414\n",
      "ON STEP: 7300\n",
      "ACCURACY: \n",
      "0.31034482\n",
      "ON STEP: 7400\n",
      "ACCURACY: \n",
      "0.31321838\n",
      "ON STEP: 7500\n",
      "ACCURACY: \n",
      "0.31034482\n",
      "ON STEP: 7600\n",
      "ACCURACY: \n",
      "0.30363986\n",
      "ON STEP: 7700\n",
      "ACCURACY: \n",
      "0.30747128\n",
      "ON STEP: 7800\n",
      "ACCURACY: \n",
      "0.30363986\n",
      "ON STEP: 7900\n",
      "ACCURACY: \n",
      "0.3218391\n",
      "ON STEP: 8000\n",
      "ACCURACY: \n",
      "0.31896552\n",
      "ON STEP: 8100\n",
      "ACCURACY: \n",
      "0.30747128\n",
      "ON STEP: 8200\n",
      "ACCURACY: \n",
      "0.30842912\n",
      "ON STEP: 8300\n",
      "ACCURACY: \n",
      "0.31226054\n",
      "ON STEP: 8400\n",
      "ACCURACY: \n",
      "0.31321838\n",
      "ON STEP: 8500\n",
      "ACCURACY: \n",
      "0.31609195\n",
      "ON STEP: 8600\n",
      "ACCURACY: \n",
      "0.3151341\n",
      "ON STEP: 8700\n",
      "ACCURACY: \n",
      "0.3170498\n",
      "ON STEP: 8800\n",
      "ACCURACY: \n",
      "0.31609195\n",
      "ON STEP: 8900\n",
      "ACCURACY: \n",
      "0.32854405\n",
      "ON STEP: 9000\n",
      "ACCURACY: \n",
      "0.3218391\n",
      "ON STEP: 9100\n",
      "ACCURACY: \n",
      "0.32088122\n",
      "ON STEP: 9200\n",
      "ACCURACY: \n",
      "0.31609195\n",
      "ON STEP: 9300\n",
      "ACCURACY: \n",
      "0.31992337\n",
      "ON STEP: 9400\n",
      "ACCURACY: \n",
      "0.3151341\n",
      "ON STEP: 9500\n",
      "ACCURACY: \n",
      "0.32662836\n",
      "ON STEP: 9600\n",
      "ACCURACY: \n",
      "0.3170498\n",
      "ON STEP: 9700\n",
      "ACCURACY: \n",
      "0.32854405\n",
      "ON STEP: 9800\n",
      "ACCURACY: \n",
      "0.3256705\n",
      "ON STEP: 9900\n",
      "ACCURACY: \n",
      "0.31896552\n"
     ]
    }
   ],
   "source": [
    "steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for j in range(steps):\n",
    "        i = np.random.randint(0, len(X_train)-10)\n",
    "\n",
    "        sess.run(train, feed_dict={x:X_train[i:i+10], y_true:y_train[i:i+10], hold_prob:0.5})\n",
    "        if j%100 == 0:\n",
    "            print(\"ON STEP: {}\".format(j))\n",
    "            print(\"ACCURACY: \")\n",
    "            \n",
    "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "            print(sess.run(acc, feed_dict={x:X_test[:10000], y_true:y_test[:10000], hold_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
